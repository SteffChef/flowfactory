(1) Perceived Benefits 

A use case is considered highly attractive when it generates clear and measurable value for the financial institution. This includes significant cost reductions and efficiency gains resulting from AI-powered automation, particularly in processes such as fraud detection, compliance, and back-office operations (McKinsey, 2023; Alonso-Robisco & Carbó, 2022). Furthermore, AI use cases that drive revenue growth through personalized products, innovative financial services, and enhanced customer targeting offer substantial business potential (Königstorfer & Thalmann, 2020). Improving customer experience through personalization and real-time interactions further strengthens the use case's value proposition, contributing to higher customer satisfaction and retention (Kaur et al., 2020; Maple et al., 2023). Moreover, the alignment of the use case with the bank's strategic objectives ensures that investments in AI directly support core business goals, thereby maximizing organizational impact (Hevner et al., 2004). 

On the other hand, use cases with low or unclear business value present significant challenges for adoption. If the financial benefits cannot be quantified or the problem addressed is not of strategic importance, the case's attractiveness diminishes (Tehrani et al., 2024). High implementation complexity without a clear return on investment further reduces feasibility and may strain financial and human resources (Prabhu Desai et al., 2025). Additionally, overestimating expected benefits relative to the organization's actual capabilities can lead to unrealistic expectations, failed projects, or resource misallocations (Lakhchini et al., 2022). 

Form 

(2) External Pressure 

In the financial sector, regulatory and compliance requirements play a crucial role in determining the feasibility of AI use cases. A favorable use case fully complies with existing regulations such as the EU AI Act and Basel standards, which govern fairness, transparency, accountability, and risk management in AI systems (BIS, 2024; EU AI Act, 2024; Basel Standards, 2024). High levels of model explainability, transparency, and fairness are essential to ensure that AI systems can be audited and understood by regulators, management, and stakeholders (Jöhnk et al., 2021; Maple et al., 2023). Proactive risk management mechanisms, such as audit trails, continuous model monitoring, and systematic bias detection, further strengthen the regulatory robustness of AI use cases (Obeng et al., 2024; Tehrani et al., 2024). 

Conversely, use cases that rely on opaque models lacking transparency and explainability face significant regulatory obstacles (Bodendorf, 2025; Cao, 2022). Non-compliance with legal standards can expose the institution to legal sanctions and reputational damage (Reddy et al., 2021). In addition, insufficient attention to data privacy, cybersecurity risks, and bias mitigation may result in operational vulnerabilities and undermine trust from customers and regulators alike (Obeng et al., 2024; Hussain & Papastathopoulos, 2022). 

Form 

(3) Organizational Readiness 

A key factor for the successful adoption of AI use cases is the organization’s internal readiness to implement and operate these solutions. High data quality, comprehensive data availability, and mature data pipelines ensure that AI models are trained on reliable and representative data sources (Tehrani et al., 2024; Prabhu Desai et al., 2025). Additionally, a qualified workforce with expertise in AI technologies, data science, regulatory compliance, and IT operations is crucial for the successful development, deployment, and oversight of AI systems (Jöhnk et al., 2021; Hussain & Papastathopoulos, 2022). Strong management commitment, well-established governance structures, and a clearly defined AI strategy provide the organizational foundation necessary to support complex AI initiatives (Hevner et al., 2004; Ali et al., 2025). 

In contrast, limited data availability, fragmented data sources, and poor data quality significantly hinder model development and performance (Biallas & O'Neill, 2020; Kaur et al., 2020). Moreover, insufficient AI skills, resistance to organizational change, and a lack of training among staff can delay or prevent successful AI adoption (Jöhnk et al., 2021; Hussain & Papastathopoulos, 2022). Legacy IT systems, fragmented infrastructures, and weak system integration further exacerbate implementation challenges and increase the complexity of operationalizing AI solutions (Cao, 2022; Maple et al., 2023). 